{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3HaSXMG6Ng6o"
   },
   "source": [
    "## Семинар 15: \"Обучение с подкреплением 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWVon7iXNg6p"
   },
   "source": [
    "ФИО: Косарев Евгений Александрович"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOynUxqkNg6q"
   },
   "source": [
    "###  FrozenLake\n",
    "\n",
    "\n",
    "<img src=\"http://vignette2.wikia.nocookie.net/riseoftheguardians/images/4/4c/Jack's_little_sister_on_the_ice.jpg/revision/latest?cb=20141218030206\" alt=\"a random image to attract attention\" style=\"width: 400px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8h8AwDAYNg6r"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "#create a single game instance\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "#start new game\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "-t-FMp-lNg6v",
    "outputId": "97221d06-dcb7-4654-ec5f-46a524831c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# display the game state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wo7ftCbANg6z"
   },
   "source": [
    "### legend\n",
    "\n",
    "![img](https://cdn-images-1.medium.com/max/800/1*MCjDzR-wfMMkS0rPqXSmKw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSHsSLxnNg60"
   },
   "source": [
    "## Задание 1.\n",
    "Подберите значения alpha и epsilon и найдите приближение оптимальной Q-функции для Frozen Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2YEY4aKNg60"
   },
   "outputs": [],
   "source": [
    "class QLearn:\n",
    "    def __init__(self, actions, epsilon=0.1, mult=1, alpha=0.2, gamma=1.0):\n",
    "        self.q = {}\n",
    "        self.epsilon = epsilon\n",
    "        self.mult = mult\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.actions = actions\n",
    "\n",
    "    def getQ(self, state, action):\n",
    "        return self.q.get((state, action), 0.0)\n",
    "\n",
    "    def chooseAction(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(self.actions)\n",
    "        else:\n",
    "            q = [self.getQ(state, a) for a in self.actions]\n",
    "            maxQ = max(q)\n",
    "            count = q.count(maxQ)\n",
    "            if count > 1:\n",
    "                best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "                i = random.choice(best)\n",
    "            else:\n",
    "                i = q.index(maxQ)\n",
    "\n",
    "            action = self.actions[i]\n",
    "        self.epsilon *= self.mult\n",
    "        return action\n",
    "\n",
    "    def learn(self, state1, action1, reward, state2):\n",
    "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
    "        oldv = self.q.get((state1, action1), None)\n",
    "        if oldv is None:\n",
    "            self.q[(state1, action1)] = reward\n",
    "        else:\n",
    "            self.q[(state1, action1)] = (1 - self.alpha)*oldv + self.alpha*(reward + self.gamma*maxqnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IKECyHEvNg63"
   },
   "outputs": [],
   "source": [
    "def run_episode_qlearn_learn(env, qlearn, gamma = 1.0, render = False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = qlearn.chooseAction(obs)\n",
    "        obs_new, reward, done, _ = env.step(action)\n",
    "        qlearn.learn(obs, action, reward, obs_new)\n",
    "        obs = obs_new\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANf4GIbINg66"
   },
   "outputs": [],
   "source": [
    "def run_episode_qlearn(env, qlearn, gamma = 1.0, render = False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = qlearn.chooseAction(obs)\n",
    "        obs_new, reward, done, _ = env.step(action)\n",
    "        obs = obs_new\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QRSzWaURNg68"
   },
   "outputs": [],
   "source": [
    "def evaluate_qlearn(env, qlearn, gamma = 1.0,  n = 100):\n",
    "    scores = [\n",
    "            run_episode_qlearn(env, qlearn, gamma = gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "tVxne-dYNg6_",
    "outputId": "ea10eb00-c15b-4b4a-8bda-29baf03822e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = 0.01, N = 500, eps = 0.05, alpha = 0.05\n",
      "result = 0.13, N = 500, eps = 0.05, alpha = 0.1\n",
      "result = 0.33, N = 500, eps = 0.05, alpha = 0.15\n",
      "result = 0.16, N = 500, eps = 0.05, alpha = 0.2\n",
      "result = 0.05, N = 500, eps = 0.05, alpha = 0.25\n",
      "result = 0.56, N = 500, eps = 0.05, alpha = 0.3\n",
      "result = 0.02, N = 500, eps = 0.1, alpha = 0.05\n",
      "result = 0.09, N = 500, eps = 0.1, alpha = 0.1\n",
      "result = 0.06, N = 500, eps = 0.1, alpha = 0.15\n",
      "result = 0.0, N = 500, eps = 0.1, alpha = 0.2\n",
      "result = 0.36, N = 500, eps = 0.1, alpha = 0.25\n",
      "result = 0.39, N = 500, eps = 0.1, alpha = 0.3\n",
      "result = 0.05, N = 500, eps = 0.15, alpha = 0.05\n",
      "result = 0.04, N = 500, eps = 0.15, alpha = 0.1\n",
      "result = 0.06, N = 500, eps = 0.15, alpha = 0.15\n",
      "result = 0.33, N = 500, eps = 0.15, alpha = 0.2\n",
      "result = 0.14, N = 500, eps = 0.15, alpha = 0.25\n",
      "result = 0.34, N = 500, eps = 0.15, alpha = 0.3\n",
      "result = 0.12, N = 500, eps = 0.2, alpha = 0.05\n",
      "result = 0.14, N = 500, eps = 0.2, alpha = 0.1\n",
      "result = 0.09, N = 500, eps = 0.2, alpha = 0.15\n",
      "result = 0.03, N = 500, eps = 0.2, alpha = 0.2\n",
      "result = 0.09, N = 500, eps = 0.2, alpha = 0.25\n",
      "result = 0.33, N = 500, eps = 0.2, alpha = 0.3\n",
      "result = 0.0, N = 500, eps = 0.25, alpha = 0.05\n",
      "result = 0.09, N = 500, eps = 0.25, alpha = 0.1\n",
      "result = 0.26, N = 500, eps = 0.25, alpha = 0.15\n",
      "result = 0.22, N = 500, eps = 0.25, alpha = 0.2\n",
      "result = 0.16, N = 500, eps = 0.25, alpha = 0.25\n",
      "result = 0.26, N = 500, eps = 0.25, alpha = 0.3\n",
      "result = 0.07, N = 500, eps = 0.3, alpha = 0.05\n",
      "result = 0.09, N = 500, eps = 0.3, alpha = 0.1\n",
      "result = 0.21, N = 500, eps = 0.3, alpha = 0.15\n",
      "result = 0.16, N = 500, eps = 0.3, alpha = 0.2\n",
      "result = 0.07, N = 500, eps = 0.3, alpha = 0.25\n",
      "result = 0.01, N = 500, eps = 0.3, alpha = 0.3\n",
      "result = 0.07, N = 1000, eps = 0.05, alpha = 0.05\n",
      "result = 0.44, N = 1000, eps = 0.05, alpha = 0.1\n",
      "result = 0.55, N = 1000, eps = 0.05, alpha = 0.15\n",
      "result = 0.47, N = 1000, eps = 0.05, alpha = 0.2\n",
      "result = 0.51, N = 1000, eps = 0.05, alpha = 0.25\n",
      "result = 0.06, N = 1000, eps = 0.05, alpha = 0.3\n",
      "result = 0.07, N = 1000, eps = 0.1, alpha = 0.05\n",
      "result = 0.31, N = 1000, eps = 0.1, alpha = 0.1\n",
      "result = 0.17, N = 1000, eps = 0.1, alpha = 0.15\n",
      "result = 0.06, N = 1000, eps = 0.1, alpha = 0.2\n",
      "result = 0.32, N = 1000, eps = 0.1, alpha = 0.25\n",
      "result = 0.06, N = 1000, eps = 0.1, alpha = 0.3\n",
      "result = 0.12, N = 1000, eps = 0.15, alpha = 0.05\n",
      "result = 0.13, N = 1000, eps = 0.15, alpha = 0.1\n",
      "result = 0.32, N = 1000, eps = 0.15, alpha = 0.15\n",
      "result = 0.33, N = 1000, eps = 0.15, alpha = 0.2\n",
      "result = 0.05, N = 1000, eps = 0.15, alpha = 0.25\n",
      "result = 0.18, N = 1000, eps = 0.15, alpha = 0.3\n",
      "result = 0.05, N = 1000, eps = 0.2, alpha = 0.05\n",
      "result = 0.26, N = 1000, eps = 0.2, alpha = 0.1\n",
      "result = 0.24, N = 1000, eps = 0.2, alpha = 0.15\n",
      "result = 0.24, N = 1000, eps = 0.2, alpha = 0.2\n",
      "result = 0.07, N = 1000, eps = 0.2, alpha = 0.25\n",
      "result = 0.1, N = 1000, eps = 0.2, alpha = 0.3\n",
      "result = 0.11, N = 1000, eps = 0.25, alpha = 0.05\n",
      "result = 0.23, N = 1000, eps = 0.25, alpha = 0.1\n",
      "result = 0.17, N = 1000, eps = 0.25, alpha = 0.15\n",
      "result = 0.2, N = 1000, eps = 0.25, alpha = 0.2\n",
      "result = 0.13, N = 1000, eps = 0.25, alpha = 0.25\n",
      "result = 0.12, N = 1000, eps = 0.25, alpha = 0.3\n",
      "result = 0.05, N = 1000, eps = 0.3, alpha = 0.05\n",
      "result = 0.2, N = 1000, eps = 0.3, alpha = 0.1\n",
      "result = 0.15, N = 1000, eps = 0.3, alpha = 0.15\n",
      "result = 0.11, N = 1000, eps = 0.3, alpha = 0.2\n",
      "result = 0.12, N = 1000, eps = 0.3, alpha = 0.25\n",
      "result = 0.05, N = 1000, eps = 0.3, alpha = 0.3\n"
     ]
    }
   ],
   "source": [
    "best_res = 0.0\n",
    "for N in [500, 1000]:\n",
    "    for epsilon in [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]:\n",
    "        for alpha in [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]:\n",
    "            qlearn = QLearn(actions=range(env.env.nA), gamma=1.0, epsilon=epsilon, mult=1, alpha=alpha)\n",
    "            for i in range(N):\n",
    "                total_reward = run_episode_qlearn_learn(env, qlearn, gamma=1.0)\n",
    "            res = evaluate_qlearn(env, qlearn)\n",
    "            if res > best_res:\n",
    "                best_params = (N, epsilon, alpha)\n",
    "                best_res = res\n",
    "            print(f'result = {res}, N = {N}, eps = {epsilon}, alpha = {alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "P1bk2QaZNg7C",
    "outputId": "23ba4c60-e819-4a50-e8cf-eb991538d797"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.56, (500, 0.05, 0.3))"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_res, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NRIbPziNg7E"
   },
   "outputs": [],
   "source": [
    "qlearn = QLearn(actions=range(env.env.nA), gamma=1.0, epsilon=0.05, mult=1, alpha=0.3)\n",
    "for i in range(1000):\n",
    "    total_reward = run_episode_qlearn_learn(env, qlearn, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8-GtVn00Ng7G",
    "outputId": "74590386-faa9-4265-ad37-4cef53f66f58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_qlearn(env, qlearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gFOXKzs5Ng7I",
    "outputId": "247d69c2-10fd-4d1f-8f54-05a1f64d9ffb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode_qlearn(env, qlearn, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "DN9qvmOYNg7K"
   },
   "source": [
    "## Задание 2.\n",
    "Обучите сеть DQN для среды http://gym.openai.com/envs/Pong-v0/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "K6r59dq4Ng7L",
    "outputId": "e385de8a-7863-492d-a645-1609c96be0ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: atari-py in /usr/local/lib/python3.6/dist-packages (0.2.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py) (1.12.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from atari-py) (1.18.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install atari-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RpmFeTl8Ng7N"
   },
   "outputs": [],
   "source": [
    "#create a single game instance\n",
    "env = gym.make(\"Pong-v0\")\n",
    "\n",
    "#start new game\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PPV2RHvUNg7P",
    "outputId": "5ab6a869-3ef6-4c90-f498-84784d8a4665"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8YHR9Po0Ng7Q",
    "outputId": "f4c354f1-d35d-40df-8015-071b570540d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(210, 160, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0m9gNGYMNg7S",
    "outputId": "e49ade91-4b1d-4dd4-c746-7a47f68e2a37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-inf, inf)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "id": "oGY8vJMpNg7T",
    "outputId": "ded3ec33-cbf4-4817-ffcc-dc9e923d2257"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fz6iXy5SNg7U"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5tXLLnfNg7W"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import random\n",
    "import math\n",
    "\n",
    "Transition = namedtuple('Transion', \n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# The class that provides memory of states and actions\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZAidhQkNg7X"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# the net itself\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels=3, n_actions=6):\n",
    "        super(DQN, self).__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 22 * 16, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255\n",
    "        x = self.convs(x)\n",
    "        x = x.reshape(x.size(0), 64 * 22 * 16)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7zu4nvUzxYYM",
    "outputId": "b6b892ee-845f-42cd-e592-1beaf15e93c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1\n",
    "EPS_END = 0.02\n",
    "EPS_DECAY = 1000000\n",
    "TARGET_UPDATE = 1000\n",
    "RENDER = False\n",
    "lr = 1e-3\n",
    "INITIAL_MEMORY = 10000\n",
    "MEMORY_SIZE = 10 * INITIAL_MEMORY\n",
    "ACTION_COUNT = 6\n",
    "BOX_SIZE = (210, 160, 3)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'{device}')\n",
    "policy_net = DQN().to(device) # активно обучаемая сеть\n",
    "target_net = DQN().to(device) # ее копия, создаваемая для того, чтобы зафиксировать веса\n",
    "# target_net синхронизируется с policy_net каждые TARGET_UPDATE шагов\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# setup optimizer\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "# initialize replay memory\n",
    "memory = ReplayMemory(MEMORY_SIZE)\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state.to(device)).max(1)[1].view(1,1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(ACTION_COUNT)]], device=device)\n",
    "\n",
    "    \n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \"\"\"\n",
    "    zip(*transitions) unzips the transitions into\n",
    "    Transition(*) creates new named tuple\n",
    "    batch.state - tuple of all the states (each state is a tensor)\n",
    "    batch.next_state - tuple of all the next states (each state is a tensor)\n",
    "    batch.reward - tuple of all the rewards (each reward is a float)\n",
    "    batch.action - tuple of all the actions (each action is an int)    \n",
    "    \"\"\"\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    actions = tuple((map(lambda a: torch.tensor([[a]], device=device), batch.action))) \n",
    "    rewards = tuple((map(lambda r: torch.tensor([r], device=device), batch.reward))) \n",
    "\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "        device=device, dtype=torch.bool)\n",
    "    \n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                       if s is not None]).to(device)\n",
    "    \n",
    "\n",
    "    state_batch = torch.cat(batch.state).to(device)\n",
    "    action_batch = torch.cat(actions)\n",
    "    reward_batch = torch.cat(rewards)\n",
    "    \n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    expected_state_action_values = reward_batch\n",
    "    expected_state_action_values[non_final_mask] = ((next_state_values[non_final_mask] * GAMMA)\n",
    "                                                    + reward_batch[non_final_mask])\n",
    "\n",
    "    \n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "def get_state(obs):\n",
    "    state = np.array(obs)\n",
    "    state = state.transpose((2, 0, 1))\n",
    "    state = torch.from_numpy(state)\n",
    "    return state.unsqueeze(0)\n",
    "\n",
    "def train(env, n_episodes, render=False):\n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        state = get_state(obs)\n",
    "        total_reward = 0.0\n",
    "        t = 0\n",
    "        while True:\n",
    "            t += 1\n",
    "            action = select_action(state) #steps_done += 1\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            if not done:\n",
    "                next_state = get_state(obs)\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            memory.push(state, action.to('cpu'), next_state, reward.to('cpu'))\n",
    "            state = next_state\n",
    "\n",
    "            if steps_done > INITIAL_MEMORY:\n",
    "                optimize_model()\n",
    "\n",
    "                if steps_done % TARGET_UPDATE == 0:\n",
    "                    target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        print('Total steps: {} \\t Episode: {}/{} \\t t = {} \\t Total reward: {}'.format(steps_done, episode,\n",
    "                                                                                     n_episodes, t, total_reward))\n",
    "    env.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "kncnzf2WSu0w",
    "outputId": "b474cbb0-1cd9-4f46-b70e-c8dc45f08c5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 1441 \t Episode: 0/20 \t t = 1441 \t Total reward: -20.0\n",
      "Total steps: 2654 \t Episode: 1/20 \t t = 1213 \t Total reward: -20.0\n",
      "Total steps: 4003 \t Episode: 2/20 \t t = 1349 \t Total reward: -18.0\n",
      "Total steps: 5365 \t Episode: 3/20 \t t = 1362 \t Total reward: -19.0\n",
      "Total steps: 6503 \t Episode: 4/20 \t t = 1138 \t Total reward: -21.0\n",
      "Total steps: 7844 \t Episode: 5/20 \t t = 1341 \t Total reward: -20.0\n",
      "Total steps: 9110 \t Episode: 6/20 \t t = 1266 \t Total reward: -20.0\n",
      "Total steps: 10282 \t Episode: 7/20 \t t = 1172 \t Total reward: -20.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2f3db5beb371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-6f75510a4262>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, n_episodes, render)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mINITIAL_MEMORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                 \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mTARGET_UPDATE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-6f75510a4262>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/neural_networks_sphere/venv/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(env, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "CUfOEFyTwznv",
    "outputId": "ab2e3215-2b9b-443a-a79f-a778ba6d65b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type DQN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(policy_net, \"dqn_pong_model\")\n",
    "policy_net = torch.load(\"dqn_pong_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nT4VZC4Ow9c5"
   },
   "outputs": [],
   "source": [
    "def test(env, n_episodes, policy, render=True):\n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        state = get_state(obs)\n",
    "        total_reward = 0.0\n",
    "        t = 0\n",
    "        while True:\n",
    "            t += 1\n",
    "            action = policy(state.to(device)).max(1)[1].view(1,1)\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(0.02)\n",
    "\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            if not done:\n",
    "                next_state = get_state(obs)\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print(\"Finished Episode {} with reward {}\".format(episode, total_reward))\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_Nwsugpnw-aY",
    "outputId": "7eed788b-d653-42ca-94d3-eef8a0487a5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Episode 0 with reward -21.0\n"
     ]
    }
   ],
   "source": [
    "test(env, 1, policy_net, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRdeNhEXyl1o"
   },
   "source": [
    "Обучал примерно 200 эпизодов, награда не менялась. Пришлось остановить, так как это уже заняло 4 часа в GoogleColab. Скорее всего, нужно обучать дольше, но дедлайн через несколько часов :(\n",
    "Возможно, перебор параметров и архитектуры сети помогли бы, но учитывая, сколько времени занял первый тест, это не получится протестировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O5A4G-jO1UFL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Practice task14, Reinforcement Learning 2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
